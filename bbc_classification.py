# -*- coding: utf-8 -*-
"""BBC_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gs8iakB2alMLCVNaYPyA2ZSFyxnNTqkN

# **Imports**
"""

# importing packages
import pandas as pd
import os
import re
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer 

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import MinMaxScaler
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.ensemble import RandomForestClassifier


"""# **Reading Data**

### ***Reading the folders***
"""

print('Reading folder')
print('_____________')

# importing files from github
# !git clone https://github.com/kundanmail55/bbc-classification

main_folder = "./bbc"
folders = ["business","entertainment","politics","sport","tech"]

os.listdir(main_folder)

# removing the README file and read all other
folderslist = [f for f in os.listdir(main_folder) if not f.startswith('README')]
folderslist

"""## ***Reading the files and adding it to dataframe with type***"""

print('Read files')
print('_____________')

news = []
n_type = []

# function to read files and processing its type
for folder in folders:
    folder_path = main_folder + '/' + folder + '/'
    print(folder_path)
    files = os.listdir(folder_path)
    for text_file in files:
        file_path = folder_path + "/" + text_file
        with open(file_path, errors='replace') as f:
            data = f.readlines()
        data = ' '.join(data)
        news.append(data)
        n_type.append(folder)

len(news)

# Add in dataframe and save to a excel sheet
news_frame = pd.DataFrame(columns=('type', 'news'))
news_frame['type'] = n_type
news_frame['news'] = news

"""# **Preprocessing**"""

print('Preprocessing')
print('_____________')

news_frame.info()

# Drop duplicate data
news_frame.drop_duplicates(subset=['type', 'news'], inplace=True)
print(news_frame.head())

news_frame['type'].value_counts()

# Associate Category names with numerical index and save it in new column category_id
news_frame['type_id'] = news_frame['type'].factorize()[0]
print(news_frame.head())

# Creating a type_id to each news type
type_dataframe = news_frame[['type_id', 'type']].drop_duplicates().sort_values('type_id')
type_dataframe

type_to_id = dict(type_dataframe.values)
id_to_type = dict(type_dataframe[['type_id', 'type']].values)
print(id_to_type)

"""# **Data Cleaning**"""

print('Data Cleaning')
print('_____________')

# Function to remove whitespaces and other characters except alphabets and converting text to smallcase
def clean_text(text):
    text = re.sub("[^a-zA-Z]", " ", text)
    text = ' '.join(text.split())
    text = text.lower()
    
    return text

news_frame['processed'] = news_frame['news'].apply(clean_text)

# function to remove stopwords
stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    return [w for w in text.split() if not w in stop_words]
  
news_frame['processed'] = news_frame['processed'].apply(lambda x: remove_stopwords(x))

lemmatizer = WordNetLemmatizer()
news_frame['processed'] = news_frame['processed'].apply(lambda x: " ".join([lemmatizer.lemmatize(item) for item in x]))
print('Lemmatization complete.')

print('Data cleaned.')

print(news_frame.head())

"""# **Feature Engineering**

### *Adding features*
"""

# Character count
news_frame['char_count'] = news_frame["processed"].apply(lambda x: sum(len(word) for word in str(x).split(" ")))

# Word count
news_frame['word_count'] = news_frame["processed"].apply(lambda x: len(str(x).split(" ")))

print(news_frame.head())

"""### *Selecting Features*"""

features = ['processed', 'char_count', 'word_count']
X = news_frame[features]
Y = news_frame['type_id']

# Splitting data
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=55)

"""### *Adding Custom Transformer*"""

class TextSelector(BaseEstimator, TransformerMixin):

    def __init__(self, key):
        self.key = key

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X[self.key]
    
class NumberSelector(BaseEstimator, TransformerMixin):

    def __init__(self, key):
        self.key = key

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X[[self.key]]

"""### *Pipelines*"""

# Processing string/text using TextSelector pipeline
preprocess_text = Pipeline([
    ('key', TextSelector(key="processed")),
    ('tfidf', TfidfVectorizer())
])

preprocess_text.fit_transform(X_train)

# Processing integer using NumberSelector
word = Pipeline([
    ('key', NumberSelector(key="word_count")),
    ('min_max', MinMaxScaler())
])

word.fit_transform(X_train)

char = Pipeline([
    ('key', NumberSelector(key="char_count")),
    ('min_max', MinMaxScaler())
])

char.fit_transform(X_train)

# Adding all feature to one using Feature Union
feature_union = FeatureUnion([
    ('preprocess_text', preprocess_text),
    ('word_len', word),
    ('char_len', char)
])

# Passing the features, Kbest and classifier in one pipeline

final_pipeline = Pipeline([
  ('features', feature_union),
  ('select', SelectKBest(score_func=chi2, k=1000)),
  ('classifier', RandomForestClassifier())
])

"""### *Training the model*"""

final_pipeline.fit(X_train, y_train)

"""### *Prediction*"""

pred = final_pipeline.predict(X_test)

"""### *Performance*"""

accuracy = accuracy_score(y_test, pred)
print('Accuracy:', accuracy)
precision, recall, f1score, support = score(y_test, pred, average='macro')
print('macro-average precision:', precision)
print('macro-average recall:', recall)
print('macro-average F1 scrore:', f1score)